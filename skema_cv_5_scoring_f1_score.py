# -*- coding: utf-8 -*-
"""SKEMA CV 5 SCORING F1 SCORE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16-JfKwz1aGdfJu2aCGpPViyAPeH25Ix1

## IMPORT LIBRARY
"""

!pip install scikit-learn pandas numpy matplotlib seaborn Sastrawi wordcloud
!pip install Sastrawi==1.0.1 --force-reinstall
import os, re, joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import log_loss
from sklearn.preprocessing import FunctionTransformer
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score, precision_score, recall_score, f1_score, make_scorer

"""## DATA ACQUISITION"""

# =========================================================
# LOAD DATA
# =========================================================
from google.colab import drive
drive.mount('/content/drive')

PATH_DATA = "/content/drive/MyDrive/DATAKU/email_spam_indo.csv"
df = pd.read_csv(PATH_DATA)
df = df.rename(columns={"Pesan":"email", "Kategori":"label"})

# CEK DUPLIKAT
print("Jumlah data sebelum hapus duplikat:", len(df))
print("Jumlah duplikat:", df.duplicated().sum())

before = df.shape[0]

dup_rows = df[df.duplicated(subset='email', keep='first')]
dup_dist = dup_rows['label'].value_counts()

df = df.drop_duplicates(subset='email')

after = df.shape[0]
print("Distribusi label duplikat yang dihapus:")
print(dup_dist)

# hapus duplikat
df = df.drop_duplicates(subset=['email'])
print("Jumlah data setelah hapus duplikat:", len(df))
print("Distribusi label:\n", df['label'].value_counts())

X = df['email']
y = df['label']

"""## EDA DATA MENTAH"""

plt.figure(figsize=(6,4))
sns.countplot(
    x="label",
    data=df,
    order=['ham', 'spam'],
    palette='Set2'
)

plt.title("Distribusi Data Email Spam vs Ham")
plt.xlabel("Label")
plt.ylabel("Jumlah Email")
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()

print("Distribusi label:\n", df['label'].value_counts())

# =========================
# EDA PANJANG EMAIL (RAW)
# =========================

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

sns.set(style="whitegrid")

# Gunakan dataframe utama
df_eda = df.copy()

# Hitung panjang email (RAW TEXT)
df_eda['email_length'] = df_eda['email'].astype(str).apply(len)

# Binning panjang email (bin terakhir terbuka)
bins = [0, 500, 1000, 2000, 3000, 5000, 10000, float('inf')]
labels = [
    "0–500", "500–1000", "1000–2000",
    "2000–3000", "3000–5000",
    "5000–10000", "10000+"
]

df_eda['length_bin'] = pd.cut(
    df_eda['email_length'],
    bins=bins,
    labels=labels
)

# =========================
# BAR CHART PER RENTANG
# =========================
plt.figure(figsize=(10,5))
df_eda['length_bin'].value_counts().sort_index().plot(kind='bar')
plt.title("Jumlah Email Berdasarkan Rentang Panjang Data Mentah")
plt.xlabel("Rentang Panjang Email (karakter)")
plt.ylabel("Jumlah Email")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

print("=== Jumlah Email per Rentang Panjang ===")
print(df_eda['length_bin'].value_counts().sort_index())

# =========================
# HISTOGRAM PANJANG EMAIL PER KELAS
# =========================
plt.figure(figsize=(10,5))
sns.histplot(
    data=df_eda,
    x='email_length',
    hue='label',
    bins=40,
    kde=False,
    element='step'
)
plt.title("Distribusi Panjang Email per Kelas Data Mentah")
plt.xlabel("Panjang Email (karakter)")
plt.ylabel("Jumlah Email")
plt.tight_layout()
plt.show()

print("\n=== Jumlah Email per Rentang Panjang per Kelas ===")
print(
    df_eda
    .groupby('label')['length_bin']
    .value_counts()
    .sort_index()
)

# Hitung panjang email
df['email_length'] = df['email'].astype(str).str.len()

# Filter email <10 karakter
email_pendek = df[df['email_length'] < 15]

print("=== Email yang panjangnya <10 karakter ===")
print(email_pendek[['label', 'email_length', 'email']])

MIN_LEN = 10
MAX_LEN = 10000

# Hitung panjang email
df['email_length'] = df['email'].astype(str).str.len()

# Email terhapus karena threshold bawah
hapus_bawah = df[df['email_length'] < MIN_LEN]

# Email terhapus karena threshold atas
hapus_atas = df[df['email_length'] > MAX_LEN]

print(f"Jumlah email terhapus karena threshold bawah (< {MIN_LEN}): {len(hapus_bawah)}")
print(f"Jumlah email terhapus karena threshold atas (> {MAX_LEN}): {len(hapus_atas)}")

before = len(df)

df = df[
    (df['email_length'] >= MIN_LEN) &
    (df['email_length'] <= MAX_LEN)
].copy()

after = len(df)

print(f"Total email terhapus: {before - after}")

df.drop(columns=['email_length'], inplace=True)

"""## DATA SPLIT"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("Train:", X_train.shape)
print("Test :", X_test.shape)
print("\nDistribusi label di data latih:\n", y_train.value_counts())

"""## DATA PREPROCESSING"""

import unicodedata
import textwrap
import re
import nltk
from nltk.corpus import stopwords
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

nltk.download('stopwords')

# STOPWORD REMOVAL
stop_factory = StopWordRemoverFactory()
stopwords_id = set(stop_factory.get_stop_words()) # Sastrawi stopwords

# Stopword Inggris (NLTK)
stopwords_en = set(stopwords.words('english'))
# STEMMING
stemmer_factory = StemmerFactory()
stemmer = stemmer_factory.create_stemmer()

# KAMUS GAUL
df_kamus = pd.read_csv("/content/drive/MyDrive/DATAKU/kamus_gaul.csv")
kamus_normalisasi = dict(zip(df_kamus['slang'], df_kamus['formal']))

kamus_normalisasi["communications"] = "komunikasi"
kamus_normalisasi["university"] = "universitas"
kamus_normalisasi["mail"] = "email"
kamus_normalisasi["file"] = "berkas"
#Stopword tambahan
additional_stopwords = {
"gue", "viagra", "per",
"dpc", "nya", "sih",
"gas", "pana", "corp", "rice", "london","dear",
"faks", "ees", "lon", "jul","rxoo","gelling","epa"
}

stopwords_all = stopwords_id.union(stopwords_en).union(additional_stopwords)

def clean_text_model(text):

    # Hilangkan enter dan double enter
    text = text.replace("\n", " ")
    text = text.replace("\r", " ")
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\r+', ' ', text)

    # Case folding : mengubah teks menjadi huruf kecil
    text = str(text).lower()

    # Data cleaning : hapus URL dan email
    text = re.sub(r'\b(?:https?://|www\.|http\b|https\b)\S*|\S+@\S+\b', ' ', text)
    text = re.sub(r'\S+@\S+', ' ', text)

    # Data cleaning : hapus header email
    text = re.sub(r'\b(from|to|cc|bcc|subject|subjek|re|fw|fwd)\b', ' ', text)

    # Data cleaning : hapus domain email
    text = re.sub(r'\b(com|net|org|id|edu|inc|co)\b', ' ', text)

    # Data Cleaning
    text = re.sub(r'\b(enron|kaminski|vince|shirley|stinson|houston|hice|adobe|john|david|hou|crenshaw|stanford|ect|ee|eb|etc)\b', ' ', text, flags=re.IGNORECASE)

    # Remove Punctition : ganti underscore / dash unicode / tanda baca aneh jadi spasi
    text = re.sub(r'[_\-‐-―]+', ' ', text)

    # Remove Punctition : hapus karakter kontrol/invisible
    text = ''.join(ch for ch in text if not unicodedata.category(ch).startswith("C"))

    # Remove Punctition : hanya huruf dan spasi
    text = re.sub(r'[^a-z\s]', ' ', text)

    # Regex Penghapusan Huruf Berulang
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # Data Cleaning : hilangkan banyak spasi
    text = re.sub(r'\s+', ' ', text).strip()

    # tokenization : memecah teks menjadi kata-kata
    words = text.split()

    # Normalisasi Kata Slang
    words = [kamus_normalisasi.get(w, w) for w in words]

    # stemming : mengubah menjadi kata bentuk dasar
    words = [stemmer.stem(w) for w in words]

    # Stopword Removal : hapus kata-kata yang tidak bermakna
    words = [w for w in words if w not in stopwords_all]
    words = [w for w in words if len(w) > 2]

    return " ".join(words)

class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self, kamus_normalisasi, stopwords_all, stemmer):
        self.kamus_normalisasi = kamus_normalisasi
        self.stopwords_all = stopwords_all
        self.stemmer = stemmer

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [clean_text_model(str(x)) for x in X]

preprocess = TextPreprocessor(kamus_normalisasi, stopwords_all, stemmer)

df_preview = pd.DataFrame({
    "Teks_Asli": X[:5],
    "Teks_Setelah_Preprocessing": preprocess.transform(X[:5])
})

df_preview

"""## EXPLORATORY DATA ANALYSIS DATA BERSIH"""

# Buat DataFrame EDA dari DATA LATIH
df_eda = pd.DataFrame({
    'email_raw': X_train.values,
    'label': y_train.values
})

df_eda['clean_text'] = preprocess.transform(df_eda['email_raw'])

plt.figure(figsize=(6,4))
sns.countplot(
    x='label',
    data=df_eda,
    palette='Set2'
)
plt.title("Distribusi Data Email Data Bersih")
plt.xlabel("Label")
plt.ylabel("Jumlah Email")
plt.grid(axis='y', linestyle='--', alpha=0.4)
plt.show()

print("Distribusi label:\n", df_eda['label'].value_counts())

# =========================
# EDA PANJANG EMAIL (SETELAH PREPROCESSING - DATA LATIH)
# =========================

# Hitung panjang email dari CLEAN TEXT
df_eda['email_length'] = df_eda['clean_text'].astype(str).apply(len)

# BIN HARUS SAMA DENGAN EDA RAW & THRESHOLD
bins = [0, 500, 1000, 2000, 3000, 5000, 10000, float('inf')]
labels = [
    "0–500", "500–1000", "1000–2000",
    "2000–3000", "3000–5000",
    "5000–10000", "10000+"
]

df_eda['length_bin'] = pd.cut(
    df_eda['email_length'],
    bins=bins,
    labels=labels
)

# =========================
# BAR CHART PER RENTANG
# =========================
plt.figure(figsize=(10,5))
df_eda['length_bin'].value_counts().sort_index().plot(kind='bar')
plt.title("Jumlah Email Berdasarkan Rentang Panjang Data Bersih")
plt.xlabel("Rentang Panjang Email (karakter)")
plt.ylabel("Jumlah Email")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

print("=== Jumlah Email per Rentang Panjang Data Bersih ===")
print(df_eda['length_bin'].value_counts().sort_index())

# =========================
# HISTOGRAM PANJANG EMAIL PER KELAS
# =========================
plt.figure(figsize=(10,5))
sns.histplot(
    data=df_eda,
    x='email_length',
    hue='label',
    bins=40,
    kde=False,
    element='step'
)
plt.title("Distribusi Panjang Email per Kelas Data Bersih")
plt.xlabel("Panjang Email (karakter)")
plt.ylabel("Jumlah Email")
plt.tight_layout()
plt.show()

print("\n=== Jumlah Email per Rentang Panjang per Kelas Data Bersih ===")
print(
    df_eda
    .groupby('label')['length_bin']
    .value_counts()
    .sort_index()
)

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# =====================================================
# WORDCLOUD MENGGUNAKAN HASIL PREPROCESSING (clean_text)
# =====================================================

for label in df_eda['label'].unique():

    text_label = " ".join(
        df_eda[df_eda['label'] == label]['clean_text']
        .dropna()
        .astype(str)
    )

    wc = WordCloud(
        width=1200,
        height=600,
        background_color='white',
        collocations=False,
        min_font_size=10
        # TIDAK pakai stopwords lagi
        # karena sudah dihapus saat preprocessing
    ).generate(text_label)

    plt.figure(figsize=(12, 6))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(
        f"WordCloud - Email {label.upper()} Data Bersih",
        fontsize=18
    )
    plt.show()

tfidf_eda = TfidfVectorizer(
    ngram_range=(1,2),
    min_df=2,
    max_df=0.9
)

tfidf_matrix = tfidf_eda.fit_transform(df_eda['clean_text'])
feature_names = tfidf_eda.get_feature_names_out()

ham_idx = df_eda.index[df_eda['label'] == 'ham'].tolist()
spam_idx = df_eda.index[df_eda['label'] == 'spam'].tolist()

# ANALISIS HAM
tfidf_ham_matrix = tfidf_matrix[ham_idx]

num_docs_ham = tfidf_ham_matrix.shape[0]  # jumlah dokumen HAM
unique_words_ham = np.sum(tfidf_ham_matrix.toarray() > 0, axis=0)
num_unique_ham = np.sum(unique_words_ham > 0)
avg_tfidf_ham = tfidf_ham_matrix.mean()

# Rata-rata tf-idf ham
avg_tfidf_ham_per_word = np.array(tfidf_ham_matrix.mean(axis=0)).flatten()
tfidf_dict_ham = dict(zip(feature_names, avg_tfidf_ham_per_word))
tfidf_sorted_ham = dict(sorted(tfidf_dict_ham.items(), key=lambda x: x[1], reverse=True))

# Top 10 kata TF-IDF HAM
top_n = 5
df_top_ham = pd.DataFrame({
    'Kata': list(tfidf_sorted_ham.keys())[:top_n],
    'Rata2_TF_IDF': list(tfidf_sorted_ham.values())[:top_n]
})

# ANALISIS SPAM
tfidf_spam_matrix = tfidf_matrix[spam_idx]

num_docs_spam = tfidf_spam_matrix.shape[0]  # jumlah dokumen SPAM
unique_words_spam = np.sum(tfidf_spam_matrix.toarray() > 0, axis=0)
num_unique_spam = np.sum(unique_words_spam > 0)
avg_tfidf_spam = tfidf_spam_matrix.mean()

# rata-rata TF-IDF tiap kata
avg_tfidf_spam_per_word = np.array(tfidf_spam_matrix.mean(axis=0)).flatten()
tfidf_dict_spam = dict(zip(feature_names, avg_tfidf_spam_per_word))
tfidf_sorted_spam = dict(sorted(tfidf_dict_spam.items(), key=lambda x: x[1], reverse=True))

# Top 10 kata TF-IDF SPAM
df_top_spam = pd.DataFrame({
    'Kata': list(tfidf_sorted_spam.keys())[:top_n],
    'Rata2_TF_IDF': list(tfidf_sorted_spam.values())[:top_n]
})

print("=== DATA TF-IDF HAM & SPAM (EDA) ===")
print(f"Jumlah dokumen HAM (row TF-IDF): {num_docs_ham}")
print(f"Jumlah dokumen SPAM (row TF-IDF): {num_docs_spam}\n")

print(f"Jumlah kata unik HAM: {num_unique_ham}")
print(f"Jumlah kata unik SPAM: {num_unique_spam}\n")

print(f"Rata-rata TF-IDF HAM: {avg_tfidf_ham}")
print(f"Rata-rata TF-IDF SPAM: {avg_tfidf_spam}\n")

print("Top 10 kata TF-IDF HAM:")
print(df_top_ham)
print("\nTop 10 kata TF-IDF SPAM:")
print(df_top_spam)

"""## TF-IDF"""

tfidf = TfidfVectorizer(
    ngram_range=(1,2),
    min_df=2,
    max_df=0.9,
    max_features=6000,
    sublinear_tf=True,
    smooth_idf=True,
    norm='l2'
)

# Fit dan transformasi data
tfidf_matrix = tfidf.fit_transform(df_eda['clean_text'])

# Ambil daftar kata / fitur
feature_names = tfidf.get_feature_names_out()

# Hitung rata-rata TF-IDF setiap kata di seluruh dokumen
avg_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()

# Buat dictionary kata -> nilai TF-IDF
tfidf_dict = dict(zip(feature_names, avg_tfidf))

# Urutkan dari nilai tertinggi
tfidf_sorted = dict(sorted(tfidf_dict.items(), key=lambda x: x[1], reverse=True))

# Tampilkan 10 kata dengan TF-IDF tertinggi
top_n = 10
print("Top TF-IDF Words:")
for word, score in list(tfidf_sorted.items())[:top_n]:
    print(f"{word}: {score:.3f}")

# Simpan ke DataFrame (untuk tabel di skripsi)
df_tfidf = pd.DataFrame({
    'Kata': list(tfidf_sorted.keys())[:top_n],
    'Nilai_TF_IDF': list(tfidf_sorted.values())[:top_n]
})

print("\nDataFrame Top TF-IDF:")
print(df_tfidf)

# Visualisasi Top TF-IDF
plt.figure(figsize=(10,6))
plt.barh(df_tfidf['Kata'][::-1], df_tfidf['Nilai_TF_IDF'][::-1], color='skyblue')
plt.xlabel("Nilai TF-IDF")
plt.title("Top TF-IDF Words")
plt.tight_layout()
plt.show()

"""## PIPELINE"""

pipeline_nb = Pipeline([
    ('preprocess', preprocess),
    ('tfidf', tfidf),
    ('model', MultinomialNB())
])

pipeline_svm = Pipeline([
    ('preprocess', preprocess),
    ('tfidf', tfidf),
    ('model', SVC())
])

"""## MODELLING"""

import time
def evaluate_pipeline(name, pipeline):
    print(f"\n=== {name} ===")
    start_train = time.time()
    pipeline.fit(X_train, y_train)
    train_time = time.time() - start_train

    start_test = time.time()
    y_pred = pipeline.predict(X_test)
    test_time = time.time() - start_test

    print("Precision (Spam) :", precision_score(y_test, y_pred, pos_label='spam'))
    print("Recall    (Spam) :", recall_score(y_test, y_pred, pos_label='spam'))
    print("F1-Score  (Spam) :", f1_score(y_test, y_pred, pos_label='spam'))
    print("Accuracy         :", accuracy_score(y_test, y_pred))
    print("Training Time    :", train_time)
    print("Testing Time     :", test_time)
    print("\nDetail:")
    print(classification_report(y_test, y_pred, zero_division=0))

    return {
        "Model": name,
        "Precision_spam": precision_score(y_test, y_pred, pos_label='spam'),
        "Recall_spam": recall_score(y_test, y_pred, pos_label='spam'),
        "F1_spam": f1_score(y_test, y_pred, pos_label='spam'),
        "Accuracy": accuracy_score(y_test, y_pred),
        "Train_time": train_time,
        "Test_time": test_time
    }

results_baseline = []
results_baseline.append(evaluate_pipeline("Naive Bayes (Baseline)", pipeline_nb))
results_baseline.append(evaluate_pipeline("SVM (Baseline)", pipeline_svm))
baseline_df = pd.DataFrame(results_baseline)
display(baseline_df)

"""##HYPERPARAMETER TUNING"""

f1_spam = make_scorer(f1_score, pos_label='spam')

param_nb = {'model__alpha':[0.1, 0.5, 1.0]}
grid_nb = GridSearchCV(pipeline_nb, param_nb, cv=5, scoring=f1_spam, n_jobs=-1)

start = time.time()
grid_nb.fit(X_train, y_train)
tune_nb = time.time() - start

best_nb = grid_nb.best_estimator_

pipeline_svm_tuned = Pipeline([
    ('preprocess', preprocess),
    ('tfidf', tfidf),
    ('model', SVC(probability=True, random_state=42))
])

param_svm = [
    {'model__kernel':['linear'],  'model__C':[0.1,1], 'model__class_weight':[None,'balanced']},
    {'model__kernel':['rbf'],     'model__C':[0.1,1], 'model__gamma':['scale'], 'model__class_weight':[None,'balanced']},
    {'model__kernel':['poly'],    'model__C':[0.1,1], 'model__degree':[2], 'model__gamma':['scale'], 'model__coef0':[0], 'model__class_weight':[None,'balanced']},
    {'model__kernel':['sigmoid'], 'model__C':[0.1,1], 'model__gamma':['scale'], 'model__coef0':[0], 'model__class_weight':[None,'balanced']}
]

grid_svm = GridSearchCV(pipeline_svm_tuned, param_svm, cv=5, scoring=f1_spam, n_jobs=-1)

start = time.time()
grid_svm.fit(X_train, y_train)
tune_svm = time.time() - start

best_svm = grid_svm.best_estimator_

print("NB tuning time:", tune_nb)
print("Best NB params:", grid_nb.best_params_)
print("SVM tuning time:", tune_svm)
print("Best SVM params:", grid_svm.best_params_)

results_tuned = []
results_tuned.append(evaluate_pipeline("Naive Bayes (Tuned)", best_nb))
results_tuned.append(evaluate_pipeline("SVM (Tuned)", best_svm))
tuned_df = pd.DataFrame(results_tuned)
display(tuned_df)

pipeline_svm_baseline_soft = Pipeline([
    ('preprocess', preprocess),
    ('tfidf', tfidf),
    ('model', SVC(probability=True))
])

ensemble_hard_baseline = VotingClassifier(
    estimators=[
        ('nb', pipeline_nb),
        ('svm', pipeline_svm)
    ],
    voting='hard'
)
ensemble_soft_baseline = VotingClassifier(
    estimators=[
        ('nb', pipeline_nb),
        ('svm', pipeline_svm_baseline_soft)
    ],
    voting='soft'
)

ensemble_hard_tuned = VotingClassifier(
    estimators=[
        ('nb', best_nb),
        ('svm', best_svm)
    ],
    voting='hard'
)
ensemble_soft_tuned = VotingClassifier(
    estimators=[
        ('nb', best_nb),
        ('svm', best_svm)
    ],
    voting='soft'
)

results = []

# Baseline
results.append(evaluate_pipeline("NB Baseline", pipeline_nb))
results.append(evaluate_pipeline("SVM Baseline", pipeline_svm))

# Ensemble Baseline
results.append(evaluate_pipeline("Ensemble Hard Baseline", ensemble_hard_baseline))
results.append(evaluate_pipeline("Ensemble Soft Baseline", ensemble_soft_baseline))

# Tuned
results.append(evaluate_pipeline("NB Tuned", best_nb))
results.append(evaluate_pipeline("SVM Tuned", best_svm))

# Ensemble Tuned
results.append(evaluate_pipeline("Ensemble Hard Tuned", ensemble_hard_tuned))
results.append(evaluate_pipeline("Ensemble Soft Tuned", ensemble_soft_tuned))

df_results = pd.DataFrame(results)
df_results

"""## ROC AUC"""

y_test_bin = y_test.map({'ham': 0, 'spam': 1})

models_roc = {
    "NB Baseline": pipeline_nb,
    "SVM Baseline": pipeline_svm,
    "NB Tuned": best_nb,
    "SVM Tuned": best_svm,
    "Ensemble Soft Baseline": ensemble_soft_baseline,
    "Ensemble Soft Tuned": ensemble_soft_tuned
}

def get_score(model, X):
    if hasattr(model, "predict_proba"):
        return model.predict_proba(X)[:, 1]
    elif hasattr(model, "decision_function"):
        return model.decision_function(X)
    else:
        raise ValueError("Model tidak mendukung ROC AUC")


# ==============================
# HITUNG ROC AUC SETIAP MODEL
# ==============================
roc_auc_results = {}

for name, model in models_roc.items():

    y_score = get_score(model, X_test)
    fpr, tpr, _ = roc_curve(y_test_bin, y_score)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.4f}')
    plt.plot([0, 1], [0, 1], linestyle='--', lw=1)

    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {name}')
    plt.legend(loc='lower right')
    plt.grid(True)
    plt.show()

models_single_baseline = {
    "Naive Bayes Baseline": pipeline_nb,
    "SVM Baseline": pipeline_svm
}

plt.figure(figsize=(8, 7))

for name, model in models_single_baseline.items():
    y_score = get_score(model, X_test)
    fpr, tpr, _ = roc_curve(y_test_bin, y_score)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Model Tunggal – Baseline')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

models_single_tuned = {
    "Naive Bayes Tuned": best_nb,
    "SVM Tuned": best_svm
}

plt.figure(figsize=(8, 7))

for name, model in models_single_tuned.items():
    y_score = get_score(model, X_test)
    fpr, tpr, _ = roc_curve(y_test_bin, y_score)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Model Tunggal – Tuned')
plt.legend(loc='lower right')
plt.grid(True)
plt.grid(True)
plt.show()

models_ensemble = {
    "Ensemble Baseline": ensemble_soft_baseline,
    "Ensemble Tuned": ensemble_soft_tuned
}

plt.figure(figsize=(8, 7))

for name, model in models_ensemble.items():
    y_score = get_score(model, X_test)
    fpr, tpr, _ = roc_curve(y_test_bin, y_score)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Ensemble Soft Voting')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# ===========================
# Eksperimen TF-IDF (cek parameter paling berpengaruh)
# ===========================

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# ===========================
# 1. Gunakan data yang sudah bersih/preprocessed
# ===========================
# Ganti 'CleanTextColumn' dengan nama kolom teks yang sudah dibersihkan
df_clean = df.copy()

# Apply preprocessing to create 'clean_text' column
df_clean['clean_text'] = preprocess.transform(df_clean['email'])

text_column = 'clean_text'

# ===========================
# 2. Konfigurasi default TF-IDF (sesuai Tabel 4.12)
# ===========================
default_params = {
    'ngram_range': (1,2),
    'min_df': 2,
    'max_df': 0.9,
    'max_features': 6000,
    'sublinear_tf': True,
    'smooth_idf': True,
    'norm': 'l2'
}

# ===========================
# 3. Fungsi untuk eksperimen satu parameter
# ===========================
def test_parameter(name, values):
    print(f"\n=== Mengecek pengaruh parameter: {name} ===")
    for val in values:
        # Salin default
        params = default_params.copy()
        # Ganti parameter yang diuji
        params[name] = val

        tfidf = TfidfVectorizer(**params)
        X_tfidf = tfidf.fit_transform(df_clean[text_column])

        # Rata-rata TF-IDF semua kata
        tfidf_means = np.array(X_tfidf.mean(axis=0)).flatten()
        avg_tfidf = tfidf_means.mean()

        print(f"{name}={val} | rata-rata TF-IDF: {avg_tfidf:.5f}")

# ===========================
# 4. Daftar parameter yang diuji
# ===========================
experiments = {
    'ngram_range': [(1,1), (1,2)],
    'min_df': [1,2],
    'max_df': [0.8,0.9],
    'max_features': [4000,6000],
    'sublinear_tf': [True, False],
    'smooth_idf': [True, False],
    'norm': ['l1','l2']
}

# ===========================
# 5. Jalankan eksperimen
# ===========================
for param_name, param_values in experiments.items():
    test_parameter(param_name, param_values)

# ===========================
# 6. Insight 10 kata teratas TF-IDF (untuk referensi)
# ===========================
tfidf = TfidfVectorizer(**default_params)
X_tfidf = tfidf.fit_transform(df_clean[text_column])

tfidf_means = np.array(X_tfidf.mean(axis=0)).flatten()
terms = np.array(tfidf.get_feature_names_out())
top_indices = tfidf_means.argsort()[::-1][:10]

print("\n10 kata teratas TF-IDF (rata-rata):")
for idx in top_indices:
    print(f"{terms[idx]}: {tfidf_means[idx]:.4f}")

"""## LOG LOSS"""

cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# SEMUA MODEL (boleh banyak)
all_models = {
    "Naive Bayes Baseline": pipeline_nb,
    "SVM Baseline": pipeline_svm,
    "Hard Voting Baseline": ensemble_hard_baseline,
    "Soft Voting Baseline": ensemble_soft_baseline,
    "Naive Bayes Tuned": best_nb,
    "SVM Tuned": best_svm,
    "Hard Voting Tuned": ensemble_hard_tuned,
    "Soft Voting Tuned": ensemble_soft_tuned
}

logloss_results = []

for name, model in all_models.items():
    # cek apakah model punya predict_proba
    if not hasattr(model, "predict_proba"):
        continue

    fold_losses = []

    for train_idx, val_idx in cv5.split(X_train, y_train):
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

        model_clone = clone(model)
        model_clone.fit(X_tr, y_tr)

        y_prob = model_clone.predict_proba(X_val)

        fold_losses.append(
            log_loss(y_val, y_prob, labels=model_clone.classes_)
        )

    logloss_results.append({
        "Model": name,
        "LogLoss_Mean": np.mean(fold_losses),
        "LogLoss_STD": np.std(fold_losses)
    })

logloss_df = pd.DataFrame(logloss_results)
display(logloss_df.sort_values(by="LogLoss_Mean"))

f1_spam = make_scorer(f1_score, pos_label='spam')
cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

models = {
    "NB Baseline": pipeline_nb,
    "SVM Baseline": pipeline_svm,
    "NB Tuned": best_nb,
    "SVM Tuned": best_svm,
    "Ensemble Hard Baseline": ensemble_hard_baseline,
    "Ensemble Soft Baseline": ensemble_soft_baseline,
    "Ensemble Hard Tuned": ensemble_hard_tuned,
    "Ensemble Soft Tuned": ensemble_soft_tuned
}

"""## STANDAR DEVIASI"""

cv_results = []

for name, model in models.items():
    scores = cross_val_score(
        model,
        X_train,
        y_train,
        cv=cv5,
        scoring=f1_spam,
        n_jobs=-1
    )

    cv_results.append({
        "Model": name,
        "Mean_F1_CV5": scores.mean(),
        "Std_F1_CV5": scores.std()
    })

    print(f"\n{name}")
    print("F1 per fold :", scores)
    print("Mean F1     :", scores.mean())
    print("Std Dev F1  :", scores.std())

# === TRAIN FINAL MODEL TERBAIK ===
best_model_name = max(cv_results, key=lambda x: x["Mean_F1_CV5"])["Model"]
best_model = models[best_model_name]

print("Model terbaik berdasarkan CV:", best_model_name)

best_model.fit(X_train, y_train)

# === EVALUASI DATA TEST ===
y_pred_test = best_model.predict(X_test)

from sklearn.metrics import classification_report, f1_score

print("\n=== HASIL EVALUASI DATA TEST ===")
print(classification_report(y_test, y_pred_test))

# F1-score yang BENAR untuk label string
print("F1-score Test (macro)   :", f1_score(y_test, y_pred_test, average='macro'))
print("F1-score Test (weighted):", f1_score(y_test, y_pred_test, average='weighted'))

# Jika ingin F1 khusus kelas spam
print("F1-score Test (spam)    :", f1_score(y_test, y_pred_test, pos_label='spam'))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd

models_uji = {
    "Naive Bayes Baseline": pipeline_nb,
    "SVM Baseline": pipeline_svm,
    "Naive Bayes Tuned": best_nb,
    "SVM Tuned": best_svm,
    "Ensemble Hard Baseline": ensemble_hard_baseline,
    "Ensemble Soft Baseline": ensemble_soft_baseline,
    "Ensemble Hard Tuned": ensemble_hard_tuned,
    "Ensemble Soft Tuned": ensemble_soft_tuned
}

from google.colab import drive
import joblib, os

drive.mount('/content/drive')

SAVE_DIR = "/content/drive/MyDrive/PENTING/model_5"
os.makedirs(SAVE_DIR, exist_ok=True)

models_to_save = {
    # Baseline
    "pipeline_nb_baseline.pkl": pipeline_nb,
    "pipeline_svm_baseline.pkl": pipeline_svm,

    # Tuned
    "pipeline_nb_tuned.pkl": best_nb,
    "pipeline_svm_tuned.pkl": best_svm,

    # Ensemble
    "ensemble_hard_baseline.pkl": ensemble_hard_baseline,
    "ensemble_soft_baseline.pkl": ensemble_soft_baseline,
    "ensemble_hard_tuned.pkl": ensemble_hard_tuned,
    "ensemble_soft_tuned.pkl": ensemble_soft_tuned,
}

for name, model in models_to_save.items():
    path = os.path.join(SAVE_DIR, name)
    joblib.dump(model, path)
    print(f"✅ Saved: {path}")